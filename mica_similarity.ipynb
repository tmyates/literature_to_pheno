{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-franklin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:51:58.736513Z",
     "start_time": "2021-12-21T14:51:58.732503Z"
    }
   },
   "outputs": [],
   "source": [
    "from ontobio.ontol_factory import OntologyFactory\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-better",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.532869Z",
     "start_time": "2021-12-21T14:45:59.147365Z"
    }
   },
   "outputs": [],
   "source": [
    "ont = OntologyFactory().create('hp.obo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-settlement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.558988Z",
     "start_time": "2021-12-21T14:46:20.535780Z"
    }
   },
   "outputs": [],
   "source": [
    "# expand: repeat hp term across set by its frequency\n",
    "def hp_exp_by_freq(model_df):\n",
    "    exp_d = []\n",
    "    for i in range(len(model_df)):\n",
    "        mod_id = model_df.loc[i]['model_id']\n",
    "        # repeat each hp term according to its freq (in nested list)\n",
    "        exp_lists = [[hp for i in range(f)] for hp, f in model_df.loc[i]['hp_freq']]\n",
    "        # convert nested list to single list\n",
    "        exp_hp = [hp for hp_list in exp_lists for hp in hp_list]\n",
    "        for hp in exp_hp:\n",
    "            exp_d.append({'model_id':mod_id, 'hp':hp})\n",
    "    return exp_d\n",
    "\n",
    "# each hp term just by unique occurence per model\n",
    "def hp_base_df(model_df):\n",
    "    base_d = []\n",
    "    for i in range(len(model_df)):\n",
    "        mod_id = model_df.loc[i]['model_id']\n",
    "        for hp,v in Counter(model_df.loc[i]['hp_base']).most_common():\n",
    "            base_d.append(({'model_id':mod_id, 'hp':hp, 'model_freq': v}))\n",
    "    return pd.DataFrame(base_d)\n",
    "\n",
    "# add ancestors for each annotated term\n",
    "def anc_freq_df(hp_freq_df):\n",
    "    anc_d = []\n",
    "    for i in range(len(hp_freq_df)):\n",
    "        mod_id = hp_freq_df.loc[i]['model_id']\n",
    "        hp_base_t = hp_freq_df.loc[i]['hp']\n",
    "        hp_anc = ont.ancestors(hp_base_t)\n",
    "        hp_anc.insert(0, hp_base_t)\n",
    "        for hp_prop in hp_anc:\n",
    "\n",
    "            anc_d.append({'model_id':mod_id, 'hp':hp_prop})\n",
    "                #     drop duplicates for each model\n",
    "    return(pd.DataFrame(anc_d).drop_duplicates())\n",
    "\n",
    "# count occurences of hp term across set and divide by no dis models to get base freq\n",
    "def ic_df(hp_freq_df):\n",
    "    n_models = len(set(hp_freq_df['model_id']))\n",
    "    base_freq_df = pd.DataFrame(hp_freq_df['hp'].value_counts()).reset_index()\n",
    "    base_freq_df = base_freq_df.rename(columns={'index':'hp', 'hp':'term_freq'})\n",
    "    base_freq_df['anno_freq'] = base_freq_df['term_freq']/n_models\n",
    "    base_freq_df['ic'] = [-math.log(i,2) for i in base_freq_df['anno_freq']]\n",
    "    return base_freq_df\n",
    "\n",
    "# mica for pair of hp terms\n",
    "def mica(hp1, hp2, anc_df):\n",
    "    anc1 = ont.ancestors(hp1)\n",
    "    anc1.insert(0, hp1)\n",
    "\n",
    "    anc2 = ont.ancestors(hp2)\n",
    "    anc2.insert(0, hp2)\n",
    "    \n",
    "    # overlapping anc terms between two hp terms\n",
    "    anc_overlap = list(set(anc1).intersection(set(anc2)))\n",
    "    \n",
    "    mica = anc_df[anc_df['hp'].isin(anc_overlap)]['ic'].max()\n",
    "    \n",
    "    return mica\n",
    "\n",
    "# df with terms of one model as rows and terms of the other as columns\n",
    "# compute mica for each pair of terms\n",
    "def mica_df(model_id1, model_id2, base_df, anc_df):\n",
    "    model1 = base_df[base_df['model_id']== model_id1].reset_index()\n",
    "    model2 = base_df[base_df['model_id']== model_id2].reset_index()\n",
    "    mica_df = pd.DataFrame(index=model1['hp'],columns=model2['hp'])\n",
    "\n",
    "    for i in range(len(mica_df.index)):\n",
    "        for j in range(len(mica_df.columns)):\n",
    "            hp1 = mica_df.index[i]\n",
    "            hp2 = mica_df.columns[j]\n",
    "            mica_df.loc[hp1,hp2] = mica(hp1,hp2,anc_df)\n",
    "        \n",
    "    return mica_df\n",
    "\n",
    "# sim max is max of all rows + max of all cols /2 \n",
    "# see PMID 31104773\n",
    "def sim_max(mica_df):\n",
    "    row_max = (mica_df.max(axis=1).values).sum()\n",
    "    col_max = (mica_df.max(axis=0).values).sum()\n",
    "    sim_max = (row_max+col_max)/2\n",
    "    return sim_max\n",
    "\n",
    "def sim_av_from_precomp(model_id1, model_id2, base_df, precomp_mica):\n",
    "\n",
    "    model1 = base_df[base_df['model_id']== model_id1].reset_index()\n",
    "    model2 = base_df[base_df['model_id']== model_id2].reset_index()\n",
    "\n",
    "    mica_cols = precomp_mica[precomp_mica['hp2'].isin(model1['hp'])]\n",
    "    mica_cols = mica_cols[mica_cols['hp1'].isin(model2['hp'])]\n",
    "    m = len(set(mica_cols['hp1']))\n",
    "    \n",
    "    mica_rows = precomp_mica[precomp_mica['hp1'].isin(model1['hp'])]\n",
    "    mica_rows = mica_rows[mica_rows['hp2'].isin(model2['hp'])]\n",
    "    n = len(set(mica_rows['hp1']))\n",
    "    \n",
    "    \n",
    "    mica_col_lst = []\n",
    "    for i in set(mica_cols['hp1']):\n",
    "        mica = mica_cols[mica_cols['hp1']==i]['mica'].max()\n",
    "        mica_col_lst.append({'hp1': i, 'mica': mica})\n",
    "    mica_col = pd.DataFrame(mica_col_lst).merge(model2, how='left',left_on='hp1',right_on='hp')\n",
    "    mica_col['weighted_mica'] = mica_col['mica']*mica_col['model_freq']\n",
    "    mica_col_mean = (mica_col['weighted_mica'].sum())/len(mica_col)\n",
    "    \n",
    "    mica_row_lst = []\n",
    "    for i in set(mica_rows['hp1']):\n",
    "        mica = mica_rows[mica_rows['hp1']==i]['mica'].max()\n",
    "        mica_row_lst.append({'hp1': i, 'mica': mica})\n",
    "    mica_row = pd.DataFrame(mica_row_lst).merge(model1, how='left',left_on='hp1',right_on='hp')\n",
    "    mica_row['weighted_mica'] = mica_row['mica']*mica_col['model_freq']\n",
    "    mica_row_mean = (mica_row['weighted_mica'].sum())/len(mica_col)\n",
    "\n",
    "\n",
    "    return 0.5*(mica_col_mean+mica_row_mean)\n",
    "\n",
    "def sim_df(base_df, precomp_mica, suffix1, suffix2):\n",
    "\n",
    "    model_ids1 = [i for i in list(dict.fromkeys(base_df['model_id'])) if suffix1 in i]\n",
    "    model_ids2 = [i for i in list(dict.fromkeys(base_df['model_id'])) if suffix2 in i]\n",
    "\n",
    "    sim_df = pd.DataFrame(index=model_ids1,columns=model_ids2)\n",
    "    \n",
    "    for i in range(len(sim_df.index)):\n",
    "        for j in range(len(sim_df.columns)):\n",
    "            model_id1 = sim_df.index[i]\n",
    "            model_id2 = sim_df.columns[j]\n",
    "            sim_df.loc[model_id1,model_id2] = sim_av_from_precomp(model_id1, model_id2, base_df, precomp_mica)\n",
    "            print (f'added sim_av {model_id1,model_id2}')\n",
    "    sim_df = sim_df.apply(pd.to_numeric)    \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-franchise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.565481Z",
     "start_time": "2021-12-21T14:46:20.561745Z"
    }
   },
   "outputs": [],
   "source": [
    "def precompute_mica(anc_base):\n",
    "    pre_lst = []\n",
    "    for i in range(len(anc_base)):\n",
    "        for j in range(len(anc_base)):\n",
    "            k = (anc_base['hp'][i], anc_base['hp'][j])\n",
    "            pre_lst.append(k)\n",
    "\n",
    "    precompute_mica = pd.DataFrame(pre_lst, columns =['hp1','hp2'])\n",
    "\n",
    "    precompute_mica['mica']  = precompute_mica.apply(lambda x: mica(x.hp1, x.hp2, anc_base), axis=1)\n",
    "    precompute_mica['mica'] = precompute_mica['mica'].fillna(-0.0)\n",
    "    \n",
    "    return precompute_mica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-browse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.569725Z",
     "start_time": "2021-12-21T14:46:20.567213Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pheno_csv(pheno_csv):\n",
    "    phen_df = pd.read_csv(pheno_csv)\n",
    "    phen_df['hp_base'] = [i.split(\",\") for i in phen_df['hp_base']]\n",
    "    return phen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-magazine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.610283Z",
     "start_time": "2021-12-21T14:46:20.571741Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_ddd = load_pheno_csv('path_to_content_text_base.csv')\n",
    "mim_ddd = load_pheno_csv('path_to_mim_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-toyota",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.657169Z",
     "start_time": "2021-12-21T14:46:20.655220Z"
    }
   },
   "outputs": [],
   "source": [
    "base_df_cont_ddd = hp_base_df(cont_ddd)\n",
    "# ic for all terms across set including propagated ancestors\n",
    "anc_base_cont_ddd = ic_df(anc_freq_df(base_df_cont_ddd))\n",
    "\n",
    "base_df_mim_ddd = hp_base_df(mim_ddd)\n",
    "# ic for all terms across set including propagated ancestors\n",
    "anc_base_mim_ddd = ic_df(anc_freq_df(base_df_mim_ddd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-pakistan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.660930Z",
     "start_time": "2021-12-21T14:46:20.658864Z"
    }
   },
   "outputs": [],
   "source": [
    "precompute_mica_cont_ddd = precompute_mica(anc_base_cont_ddd)\n",
    "pickle.dump(precompute_mica_cont_ddd,open('path_to_precompute_cont_ddd.p','wb'))\n",
    "\n",
    "precompute_mica_mim_ddd = precompute_mica(anc_base_mim_ddd)\n",
    "pickle.dump(precompute_mica_mim_ddd,open('path_to_precompute_mim_ddd.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-classification",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:46:20.665327Z",
     "start_time": "2021-12-21T14:46:20.663069Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_df_cont_ddd = sim_df(base_df_cont_ddd, precompute_mica_cont_ddd, '_content', '_ddd')\n",
    "pickle.dump(sim_df_cont_ddd, open('path_to_cont_ddd_simdf.p','wb'))\n",
    "\n",
    "sim_df_mim_ddd = sim_df(base_df_mim_ddd, precompute_mica_mim_ddd, '_mim', '_ddd')\n",
    "pickle.dump(sim_df_mim_ddd, open('path_to_mim_ddd_simdf.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-swiss",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
